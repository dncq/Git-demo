{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37c3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AdamW, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8d1ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09248f3dcbf54ecd97c1041e36d56931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a527e948844e37ba0fc58beef311fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6438b97bcd442edb5472fd0583104f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/164k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a4573f268744268bbeb8e5bb77a987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/156k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25c1d4587154eccbe46264c5dbd8ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1977e83a697c419e98911df6dfd7f0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e3d28aea634c74bae4b274f32df51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Multi30k dataset\n",
    "# dataset = load_dataset(\"bentrevett/multi30k\", split=\"train[:10000]\")  # Load only a subset for demonstration\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")  # Load only a subset for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25320ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 29000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1014\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'de'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091af4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'de'],\n",
       "    num_rows: 29000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9316239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize T5 tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5422408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_data(example):\n",
    "#     source_text = example[\"en\"]\n",
    "#     target_text = example[\"de\"]\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         source_text,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=128,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     tokenized_targets = tokenizer(\n",
    "#         target_text,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=128,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     return {\n",
    "#         \"input_ids\": tokenized_inputs.input_ids.flatten(),\n",
    "#         \"attention_mask\": tokenized_inputs.attention_mask.flatten(),\n",
    "#         \"labels\": tokenized_targets.input_ids.flatten(),\n",
    "#         \"labels_attention_mask\": tokenized_targets.attention_mask.flatten(),\n",
    "#     }\n",
    "\n",
    "\n",
    "# def tokenize_data(example):\n",
    "#     source_text = example[\"en\"]\n",
    "#     target_text = example[\"de\"]\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         source_text,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=128,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     tokenized_targets = tokenizer(\n",
    "#         target_text,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=128,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     return {\n",
    "#         \"input_ids\": tokenized_inputs.input_ids[0],\n",
    "#         \"attention_mask\": tokenized_inputs.attention_mask[0],\n",
    "#         \"labels\": tokenized_targets.input_ids[0],\n",
    "#         \"labels_attention_mask\": tokenized_targets.attention_mask[0],\n",
    "#     }\n",
    "\n",
    "\n",
    "# Tokenize and preprocess data\n",
    "def tokenize_data(batch):\n",
    "    src_texts = batch[\"en\"]\n",
    "    tgt_texts = batch[\"de\"]\n",
    "    tokenized_batch = tokenizer.prepare_seq2seq_batch(src_texts, tgt_texts, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": tokenized_batch.input_ids,\n",
    "        \"attention_mask\": tokenized_batch.attention_mask,\n",
    "        \"labels\": tokenized_batch.labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4af8ff66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f687d36203243aca1ddedd2396fd42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3986: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439d5221aec24af39bb0c56c93016912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7ab14a8c2248b9bb1a33c20eeb4d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize_data, batched=True)\n",
    "train_dataloader = DataLoader(dataset['train'], batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425a5a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c60fcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 15.397992134094238\n",
      "Epoch 1, Loss: 13.578856468200684\n",
      "Epoch 1, Loss: 12.593639373779297\n",
      "Epoch 1, Loss: 12.532017707824707\n",
      "Epoch 1, Loss: 10.71521282196045\n",
      "Epoch 1, Loss: 11.551669120788574\n",
      "Epoch 1, Loss: 9.687947273254395\n",
      "Epoch 1, Loss: 8.377568244934082\n",
      "Epoch 1, Loss: 8.626347541809082\n",
      "Epoch 1, Loss: 5.613637447357178\n",
      "Epoch 1, Loss: 5.17933464050293\n",
      "Epoch 1, Loss: 6.232644557952881\n",
      "Epoch 1, Loss: 3.7043616771698\n",
      "Epoch 1, Loss: 5.3638105392456055\n",
      "Epoch 1, Loss: 3.612170696258545\n",
      "Epoch 1, Loss: 3.1822938919067383\n",
      "Epoch 1, Loss: 3.062915325164795\n",
      "Epoch 1, Loss: 2.068197011947632\n",
      "Epoch 1, Loss: 2.258237361907959\n",
      "Epoch 1, Loss: 2.4222443103790283\n",
      "Epoch 1, Loss: 2.2555246353149414\n",
      "Epoch 1, Loss: 2.298738718032837\n",
      "Epoch 1, Loss: 2.8426969051361084\n",
      "Epoch 1, Loss: 2.138777732849121\n",
      "Epoch 1, Loss: 2.4314329624176025\n",
      "Epoch 1, Loss: 2.6504244804382324\n",
      "Epoch 1, Loss: 2.6182641983032227\n",
      "Epoch 1, Loss: 2.2058684825897217\n",
      "Epoch 1, Loss: 2.019033432006836\n",
      "Epoch 1, Loss: 2.049898386001587\n",
      "Epoch 1, Loss: 2.1552202701568604\n",
      "Epoch 1, Loss: 2.031942367553711\n",
      "Epoch 1, Loss: 2.1993703842163086\n",
      "Epoch 1, Loss: 1.746904969215393\n",
      "Epoch 1, Loss: 2.184556722640991\n",
      "Epoch 1, Loss: 1.850521445274353\n",
      "Epoch 1, Loss: 1.8546580076217651\n",
      "Epoch 1, Loss: 2.2196946144104004\n",
      "Epoch 1, Loss: 2.079773187637329\n",
      "Epoch 1, Loss: 1.6884866952896118\n",
      "Epoch 1, Loss: 2.55474591255188\n",
      "Epoch 1, Loss: 2.02984619140625\n",
      "Epoch 1, Loss: 1.9754363298416138\n",
      "Epoch 1, Loss: 1.430301308631897\n",
      "Epoch 1, Loss: 1.719007968902588\n",
      "Epoch 1, Loss: 2.10103702545166\n",
      "Epoch 1, Loss: 2.0596532821655273\n",
      "Epoch 1, Loss: 2.5487165451049805\n",
      "Epoch 1, Loss: 1.8736114501953125\n",
      "Epoch 1, Loss: 2.091269016265869\n",
      "Epoch 1, Loss: 1.826187014579773\n",
      "Epoch 1, Loss: 2.024528980255127\n",
      "Epoch 1, Loss: 1.683840274810791\n",
      "Epoch 1, Loss: 1.688191533088684\n",
      "Epoch 1, Loss: 1.5553027391433716\n",
      "Epoch 1, Loss: 2.138223648071289\n",
      "Epoch 1, Loss: 1.658096432685852\n",
      "Epoch 1, Loss: 1.7866123914718628\n",
      "Epoch 1, Loss: 1.3057698011398315\n",
      "Epoch 1, Loss: 1.5960118770599365\n",
      "Epoch 1, Loss: 1.738011360168457\n",
      "Epoch 1, Loss: 1.37052583694458\n",
      "Epoch 1, Loss: 1.6611355543136597\n",
      "Epoch 1, Loss: 1.7377960681915283\n",
      "Epoch 1, Loss: 1.713783860206604\n",
      "Epoch 1, Loss: 2.052639961242676\n",
      "Epoch 1, Loss: 1.917495608329773\n",
      "Epoch 1, Loss: 1.3372589349746704\n",
      "Epoch 1, Loss: 2.00506329536438\n",
      "Epoch 1, Loss: 1.8097368478775024\n",
      "Epoch 1, Loss: 1.555249810218811\n",
      "Epoch 1, Loss: 1.5466207265853882\n",
      "Epoch 1, Loss: 1.3725284337997437\n",
      "Epoch 1, Loss: 1.315151333808899\n",
      "Epoch 1, Loss: 1.2948766946792603\n",
      "Epoch 1, Loss: 1.8657565116882324\n",
      "Epoch 1, Loss: 1.7032856941223145\n",
      "Epoch 1, Loss: 1.6902985572814941\n",
      "Epoch 1, Loss: 1.6551319360733032\n",
      "Epoch 1, Loss: 1.3252800703048706\n",
      "Epoch 1, Loss: 1.49858558177948\n",
      "Epoch 1, Loss: 2.0129740238189697\n",
      "Epoch 1, Loss: 1.525426983833313\n",
      "Epoch 1, Loss: 1.8102126121520996\n",
      "Epoch 1, Loss: 1.4108307361602783\n",
      "Epoch 1, Loss: 1.3464068174362183\n",
      "Epoch 1, Loss: 1.783556580543518\n",
      "Epoch 1, Loss: 1.4375207424163818\n",
      "Epoch 1, Loss: 1.8533366918563843\n",
      "Epoch 1, Loss: 1.274756669998169\n",
      "Epoch 1, Loss: 1.427101969718933\n",
      "Epoch 1, Loss: 1.2414324283599854\n",
      "Epoch 1, Loss: 1.606770396232605\n",
      "Epoch 1, Loss: 1.3400627374649048\n",
      "Epoch 1, Loss: 1.2370109558105469\n",
      "Epoch 1, Loss: 1.476840853691101\n",
      "Epoch 1, Loss: 2.5289371013641357\n",
      "Epoch 1, Loss: 1.4500057697296143\n",
      "Epoch 1, Loss: 1.5439282655715942\n",
      "Epoch 1, Loss: 1.0236766338348389\n",
      "Epoch 1, Loss: 1.5123456716537476\n",
      "Epoch 1, Loss: 1.2788792848587036\n",
      "Epoch 1, Loss: 1.393729567527771\n",
      "Epoch 1, Loss: 1.41374933719635\n",
      "Epoch 1, Loss: 1.2740273475646973\n",
      "Epoch 1, Loss: 1.4569453001022339\n",
      "Epoch 1, Loss: 1.6034208536148071\n",
      "Epoch 1, Loss: 1.4161956310272217\n",
      "Epoch 1, Loss: 1.397492527961731\n",
      "Epoch 1, Loss: 1.417973279953003\n",
      "Epoch 1, Loss: 1.4093050956726074\n",
      "Epoch 1, Loss: 1.2762558460235596\n",
      "Epoch 1, Loss: 1.5142557621002197\n",
      "Epoch 1, Loss: 1.2708845138549805\n",
      "Epoch 1, Loss: 1.5663598775863647\n",
      "Epoch 1, Loss: 1.435132384300232\n",
      "Epoch 1, Loss: 1.377406358718872\n",
      "Epoch 1, Loss: 1.2904351949691772\n",
      "Epoch 1, Loss: 1.2488815784454346\n",
      "Epoch 1, Loss: 1.2802836894989014\n",
      "Epoch 1, Loss: 1.3556902408599854\n",
      "Epoch 1, Loss: 1.3481054306030273\n",
      "Epoch 1, Loss: 1.1050199270248413\n",
      "Epoch 1, Loss: 1.8076319694519043\n",
      "Epoch 1, Loss: 1.3301507234573364\n",
      "Epoch 1, Loss: 1.1401934623718262\n",
      "Epoch 1, Loss: 1.432555913925171\n",
      "Epoch 1, Loss: 1.009427547454834\n",
      "Epoch 1, Loss: 1.3193690776824951\n",
      "Epoch 1, Loss: 1.2488172054290771\n",
      "Epoch 1, Loss: 1.1701651811599731\n",
      "Epoch 1, Loss: 1.1756596565246582\n",
      "Epoch 1, Loss: 1.4224210977554321\n",
      "Epoch 1, Loss: 1.6519883871078491\n",
      "Epoch 1, Loss: 1.7484928369522095\n",
      "Epoch 1, Loss: 1.5719561576843262\n",
      "Epoch 1, Loss: 1.468634009361267\n",
      "Epoch 1, Loss: 1.175517201423645\n",
      "Epoch 1, Loss: 1.4190847873687744\n",
      "Epoch 1, Loss: 1.6280065774917603\n",
      "Epoch 1, Loss: 1.3362269401550293\n",
      "Epoch 1, Loss: 1.3524328470230103\n",
      "Epoch 1, Loss: 1.0045946836471558\n",
      "Epoch 1, Loss: 1.1416149139404297\n",
      "Epoch 1, Loss: 1.2840781211853027\n",
      "Epoch 1, Loss: 1.2845990657806396\n",
      "Epoch 1, Loss: 1.0326536893844604\n",
      "Epoch 1, Loss: 1.499160647392273\n",
      "Epoch 1, Loss: 1.4381027221679688\n",
      "Epoch 1, Loss: 1.2107787132263184\n",
      "Epoch 1, Loss: 1.400146245956421\n",
      "Epoch 1, Loss: 1.3513022661209106\n",
      "Epoch 1, Loss: 1.5324021577835083\n",
      "Epoch 1, Loss: 1.2009501457214355\n",
      "Epoch 1, Loss: 1.1508578062057495\n",
      "Epoch 1, Loss: 1.2531605958938599\n",
      "Epoch 1, Loss: 1.6114581823349\n",
      "Epoch 1, Loss: 1.0105619430541992\n",
      "Epoch 1, Loss: 1.194854497909546\n",
      "Epoch 1, Loss: 1.2186613082885742\n",
      "Epoch 1, Loss: 1.4346849918365479\n",
      "Epoch 1, Loss: 1.4042510986328125\n",
      "Epoch 1, Loss: 1.1378628015518188\n",
      "Epoch 1, Loss: 1.343628168106079\n",
      "Epoch 1, Loss: 1.1170940399169922\n",
      "Epoch 1, Loss: 1.4712446928024292\n",
      "Epoch 1, Loss: 1.2554892301559448\n",
      "Epoch 1, Loss: 1.6033408641815186\n",
      "Epoch 1, Loss: 1.2918461561203003\n",
      "Epoch 1, Loss: 1.3128315210342407\n",
      "Epoch 1, Loss: 1.2104978561401367\n",
      "Epoch 1, Loss: 1.1857826709747314\n",
      "Epoch 1, Loss: 1.5484482049942017\n",
      "Epoch 1, Loss: 1.035527229309082\n",
      "Epoch 1, Loss: 1.193568468093872\n",
      "Epoch 1, Loss: 1.3236624002456665\n",
      "Epoch 1, Loss: 1.285378336906433\n",
      "Epoch 1, Loss: 1.1192164421081543\n",
      "Epoch 1, Loss: 1.1268393993377686\n",
      "Epoch 1, Loss: 1.3261996507644653\n",
      "Epoch 1, Loss: 1.1065387725830078\n",
      "Epoch 1, Loss: 1.2505743503570557\n",
      "Epoch 1, Loss: 1.2465859651565552\n",
      "Epoch 1, Loss: 1.126410961151123\n",
      "Epoch 1, Loss: 1.4013216495513916\n",
      "Epoch 1, Loss: 1.2771414518356323\n",
      "Epoch 1, Loss: 1.2328166961669922\n",
      "Epoch 1, Loss: 1.0431722402572632\n",
      "Epoch 1, Loss: 2.0227694511413574\n",
      "Epoch 1, Loss: 1.352461338043213\n",
      "Epoch 1, Loss: 1.0557904243469238\n",
      "Epoch 1, Loss: 1.20693039894104\n",
      "Epoch 1, Loss: 1.5283535718917847\n",
      "Epoch 1, Loss: 1.2906562089920044\n",
      "Epoch 1, Loss: 1.3615717887878418\n",
      "Epoch 1, Loss: 1.2276026010513306\n",
      "Epoch 1, Loss: 1.192659616470337\n",
      "Epoch 1, Loss: 1.1652278900146484\n",
      "Epoch 1, Loss: 1.1572808027267456\n",
      "Epoch 1, Loss: 1.2176051139831543\n",
      "Epoch 1, Loss: 1.2858339548110962\n",
      "Epoch 1, Loss: 1.3641432523727417\n",
      "Epoch 1, Loss: 1.4151413440704346\n",
      "Epoch 1, Loss: 1.0881956815719604\n",
      "Epoch 1, Loss: 1.4454320669174194\n",
      "Epoch 1, Loss: 1.09761643409729\n",
      "Epoch 1, Loss: 1.5986956357955933\n",
      "Epoch 1, Loss: 1.0932321548461914\n",
      "Epoch 1, Loss: 1.3157448768615723\n",
      "Epoch 1, Loss: 1.1602139472961426\n",
      "Epoch 1, Loss: 1.0293203592300415\n",
      "Epoch 1, Loss: 1.376538634300232\n",
      "Epoch 1, Loss: 1.2104154825210571\n",
      "Epoch 1, Loss: 1.401726484298706\n",
      "Epoch 1, Loss: 1.423967719078064\n",
      "Epoch 1, Loss: 1.0624157190322876\n",
      "Epoch 1, Loss: 1.7771382331848145\n",
      "Epoch 1, Loss: 1.0848029851913452\n",
      "Epoch 1, Loss: 1.1405482292175293\n",
      "Epoch 1, Loss: 1.3563283681869507\n",
      "Epoch 1, Loss: 1.255187749862671\n",
      "Epoch 1, Loss: 1.415734052658081\n",
      "Epoch 1, Loss: 1.147408366203308\n",
      "Epoch 1, Loss: 1.3206415176391602\n",
      "Epoch 1, Loss: 1.3918588161468506\n",
      "Epoch 1, Loss: 1.5154330730438232\n",
      "Epoch 1, Loss: 1.3386584520339966\n",
      "Epoch 1, Loss: 1.175593376159668\n",
      "Epoch 1, Loss: 1.389643669128418\n",
      "Epoch 1, Loss: 1.2300935983657837\n",
      "Epoch 1, Loss: 1.095382809638977\n",
      "Epoch 1, Loss: 1.1584362983703613\n",
      "Epoch 1, Loss: 0.9655728340148926\n",
      "Epoch 1, Loss: 1.6001262664794922\n",
      "Epoch 1, Loss: 1.4114407300949097\n",
      "Epoch 1, Loss: 1.0699108839035034\n",
      "Epoch 1, Loss: 1.195258617401123\n",
      "Epoch 1, Loss: 1.0359299182891846\n",
      "Epoch 1, Loss: 1.0766515731811523\n",
      "Epoch 1, Loss: 1.0456942319869995\n",
      "Epoch 1, Loss: 1.1588704586029053\n",
      "Epoch 1, Loss: 1.2867333889007568\n",
      "Epoch 1, Loss: 1.0993049144744873\n",
      "Epoch 1, Loss: 1.1607154607772827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.130643606185913\n",
      "Epoch 1, Loss: 1.091322898864746\n",
      "Epoch 1, Loss: 1.3831450939178467\n",
      "Epoch 1, Loss: 1.1916652917861938\n",
      "Epoch 1, Loss: 1.2966530323028564\n",
      "Epoch 1, Loss: 1.112549066543579\n",
      "Epoch 1, Loss: 1.051073431968689\n",
      "Epoch 1, Loss: 1.322925090789795\n",
      "Epoch 1, Loss: 1.2747650146484375\n",
      "Epoch 1, Loss: 1.0510892868041992\n",
      "Epoch 1, Loss: 1.1529440879821777\n",
      "Epoch 1, Loss: 1.256658911705017\n",
      "Epoch 1, Loss: 1.4859212636947632\n",
      "Epoch 1, Loss: 1.1455434560775757\n",
      "Epoch 1, Loss: 1.141351342201233\n",
      "Epoch 1, Loss: 1.034354329109192\n",
      "Epoch 1, Loss: 1.2325009107589722\n",
      "Epoch 1, Loss: 1.3514087200164795\n",
      "Epoch 1, Loss: 1.028560757637024\n",
      "Epoch 1, Loss: 1.5896151065826416\n",
      "Epoch 1, Loss: 1.3591983318328857\n",
      "Epoch 1, Loss: 0.9556340575218201\n",
      "Epoch 1, Loss: 1.4168521165847778\n",
      "Epoch 1, Loss: 0.9899098873138428\n",
      "Epoch 1, Loss: 1.1894758939743042\n",
      "Epoch 1, Loss: 1.0382803678512573\n",
      "Epoch 1, Loss: 1.07610285282135\n",
      "Epoch 1, Loss: 1.2287604808807373\n",
      "Epoch 1, Loss: 1.3230339288711548\n",
      "Epoch 1, Loss: 0.9602975249290466\n",
      "Epoch 1, Loss: 1.252234935760498\n",
      "Epoch 1, Loss: 1.1608431339263916\n",
      "Epoch 1, Loss: 1.0695879459381104\n",
      "Epoch 1, Loss: 1.1197922229766846\n",
      "Epoch 1, Loss: 1.1850038766860962\n",
      "Epoch 1, Loss: 1.212446928024292\n",
      "Epoch 1, Loss: 2.018265724182129\n",
      "Epoch 1, Loss: 1.071189522743225\n",
      "Epoch 1, Loss: 1.1188132762908936\n",
      "Epoch 1, Loss: 1.206635594367981\n",
      "Epoch 1, Loss: 1.1249390840530396\n",
      "Epoch 1, Loss: 1.3886646032333374\n",
      "Epoch 1, Loss: 1.1782335042953491\n",
      "Epoch 1, Loss: 1.4000431299209595\n",
      "Epoch 1, Loss: 1.073779821395874\n",
      "Epoch 1, Loss: 1.3470656871795654\n",
      "Epoch 1, Loss: 1.2444275617599487\n",
      "Epoch 1, Loss: 1.094494342803955\n",
      "Epoch 1, Loss: 1.3128689527511597\n",
      "Epoch 1, Loss: 1.4539144039154053\n",
      "Epoch 1, Loss: 1.4110718965530396\n",
      "Epoch 1, Loss: 1.141629934310913\n",
      "Epoch 1, Loss: 1.2519675493240356\n",
      "Epoch 1, Loss: 0.9360306262969971\n",
      "Epoch 1, Loss: 0.9672945737838745\n",
      "Epoch 1, Loss: 1.3274275064468384\n",
      "Epoch 1, Loss: 1.188518762588501\n",
      "Epoch 1, Loss: 1.1061393022537231\n",
      "Epoch 1, Loss: 0.9976171255111694\n",
      "Epoch 1, Loss: 1.0978434085845947\n",
      "Epoch 1, Loss: 1.097312092781067\n",
      "Epoch 1, Loss: 1.1177924871444702\n",
      "Epoch 1, Loss: 1.1622933149337769\n",
      "Epoch 1, Loss: 1.0419961214065552\n",
      "Epoch 1, Loss: 0.9166955947875977\n",
      "Epoch 1, Loss: 1.2227556705474854\n",
      "Epoch 1, Loss: 1.2372347116470337\n",
      "Epoch 1, Loss: 1.1792492866516113\n",
      "Epoch 1, Loss: 0.9650271534919739\n",
      "Epoch 1, Loss: 1.3091979026794434\n",
      "Epoch 1, Loss: 1.4732670783996582\n",
      "Epoch 1, Loss: 1.1841509342193604\n",
      "Epoch 1, Loss: 1.1387808322906494\n",
      "Epoch 1, Loss: 1.5941585302352905\n",
      "Epoch 1, Loss: 0.9870017766952515\n",
      "Epoch 1, Loss: 0.9511170387268066\n",
      "Epoch 1, Loss: 1.3553214073181152\n",
      "Epoch 1, Loss: 1.5945210456848145\n",
      "Epoch 1, Loss: 1.154930591583252\n",
      "Epoch 1, Loss: 1.0865936279296875\n",
      "Epoch 1, Loss: 1.1254541873931885\n",
      "Epoch 1, Loss: 1.3559448719024658\n",
      "Epoch 1, Loss: 1.0881174802780151\n",
      "Epoch 1, Loss: 1.1372321844100952\n",
      "Epoch 1, Loss: 1.0471818447113037\n",
      "Epoch 1, Loss: 1.2899829149246216\n",
      "Epoch 1, Loss: 1.1102792024612427\n",
      "Epoch 1, Loss: 1.3585718870162964\n",
      "Epoch 1, Loss: 1.0891873836517334\n",
      "Epoch 1, Loss: 0.9284542798995972\n",
      "Epoch 1, Loss: 1.0747416019439697\n",
      "Epoch 1, Loss: 1.0771727561950684\n",
      "Epoch 1, Loss: 0.8960618376731873\n",
      "Epoch 1, Loss: 1.2056152820587158\n",
      "Epoch 1, Loss: 1.1683257818222046\n",
      "Epoch 1, Loss: 0.9123667478561401\n",
      "Epoch 1, Loss: 1.4044849872589111\n",
      "Epoch 1, Loss: 1.4217097759246826\n",
      "Epoch 1, Loss: 1.1028976440429688\n",
      "Epoch 1, Loss: 1.1541398763656616\n",
      "Epoch 1, Loss: 1.371071457862854\n",
      "Epoch 1, Loss: 1.1431593894958496\n",
      "Epoch 1, Loss: 1.1674582958221436\n",
      "Epoch 1, Loss: 1.0364049673080444\n",
      "Epoch 1, Loss: 1.1382737159729004\n",
      "Epoch 1, Loss: 1.1920726299285889\n",
      "Epoch 1, Loss: 1.0976016521453857\n",
      "Epoch 1, Loss: 1.6142003536224365\n",
      "Epoch 1, Loss: 1.3432891368865967\n",
      "Epoch 1, Loss: 1.1769003868103027\n",
      "Epoch 1, Loss: 1.2511796951293945\n",
      "Epoch 1, Loss: 1.0528900623321533\n",
      "Epoch 1, Loss: 1.3119405508041382\n",
      "Epoch 1, Loss: 1.226391315460205\n",
      "Epoch 1, Loss: 1.1383260488510132\n",
      "Epoch 1, Loss: 1.192142367362976\n",
      "Epoch 1, Loss: 1.2382888793945312\n",
      "Epoch 1, Loss: 1.1815787553787231\n",
      "Epoch 1, Loss: 1.3021482229232788\n",
      "Epoch 1, Loss: 1.250544548034668\n",
      "Epoch 1, Loss: 0.9124822616577148\n",
      "Epoch 1, Loss: 0.9900622367858887\n",
      "Epoch 1, Loss: 0.9781067371368408\n",
      "Epoch 1, Loss: 1.1292920112609863\n",
      "Epoch 1, Loss: 1.2789781093597412\n",
      "Epoch 1, Loss: 1.0627506971359253\n",
      "Epoch 1, Loss: 1.3264387845993042\n",
      "Epoch 1, Loss: 1.1338976621627808\n",
      "Epoch 1, Loss: 0.951927661895752\n",
      "Epoch 1, Loss: 0.8533187508583069\n",
      "Epoch 1, Loss: 1.0917131900787354\n",
      "Epoch 1, Loss: 1.2291046380996704\n",
      "Epoch 1, Loss: 1.084709644317627\n",
      "Epoch 1, Loss: 1.0474151372909546\n",
      "Epoch 1, Loss: 0.9722375869750977\n",
      "Epoch 1, Loss: 1.2791575193405151\n",
      "Epoch 1, Loss: 1.2222254276275635\n",
      "Epoch 1, Loss: 1.1357671022415161\n",
      "Epoch 1, Loss: 0.9361894130706787\n",
      "Epoch 1, Loss: 1.367775559425354\n",
      "Epoch 1, Loss: 1.5030723810195923\n",
      "Epoch 1, Loss: 0.9110473394393921\n",
      "Epoch 1, Loss: 0.9717164635658264\n",
      "Epoch 1, Loss: 1.1079307794570923\n",
      "Epoch 1, Loss: 1.060786247253418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17796/992740556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         )\n\u001b[0;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 492\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# for epoch in range(3):  # Adjust number of epochs as needed\n",
    "#     for batch in train_dataloader:\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             labels=labels,\n",
    "#         )\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "for epoch in range(1):  # Adjust number of epochs as needed\n",
    "    for batch in train_dataloader:\n",
    "        # Convert each sequence in the batch to tensors and move them to device\n",
    "        input_ids = torch.tensor([item for sublist in batch[\"input_ids\"] for item in sublist]).to(device)\n",
    "        attention_mask = torch.tensor([item for sublist in batch[\"attention_mask\"] for item in sublist]).to(device)\n",
    "        labels = torch.tensor([item for sublist in batch[\"labels\"] for item in sublist]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids.unsqueeze(0),  \n",
    "            attention_mask=attention_mask.unsqueeze(0),  \n",
    "            labels=labels.unsqueeze(0),  \n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ad0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model.save_pretrained(\"model_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
